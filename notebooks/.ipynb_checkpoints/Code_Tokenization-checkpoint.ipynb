{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81123ff8",
   "metadata": {},
   "source": [
    "# Code Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c285ac",
   "metadata": {},
   "source": [
    "### This notebook explores the code tokenization methodology implemented by SentencePiece (https://github.com/google/sentencepiece).\n",
    "\n",
    "### SentencePiece\n",
    "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for neural network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g. byte-pair-encoding (BPE)) and unigram language model with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\n",
    "\n",
    "### Characteristics of SentencePiece\n",
    "1) The number of unique tokens is predetermined  \n",
    "Neural Machine Translation models typically operate with a fixed vocabulary. Unlike most unsupervised word segmentation algorithms, which assume an infinite vocabulary, SentencePiece trains the segmentation model such that the final vocabulary size is fixed, e.g., 8K, 16K, 32K.  \n",
    "2) Trains from raw sentences  \n",
    "Previous sub-word implementations assume that the input sentences are pre-tokenized. This constraint was required for efficient training, but makes the preprocessing complicated as we have to run language dependent tokenizers in advance. The implementation of SentencePiece is fast enough to train the model from raw sentences.  \n",
    "3) Subword regularization and BPE-dropout  \n",
    "Subword regularization and BPE-dropout are simple regularization methods that virtually augment training data with on-the-fly subword sampling, which helps to improve the accuracy as well as robustness of NMT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26b350",
   "metadata": {},
   "source": [
    "### Part 1: Training the SentencePiece Model\n",
    "We intend to train a code tokenizer for the pre-training dataset and the fine-tuning dataset. Specifically, 1) we will pre-process the above two datasets by removing comments; 2) we will build an one-sentence-per-line raw corpus (txt file) based on the two pre-processed datasets by selecting all functions and programs which can be parsed into ASTs successfully; 3) we will use the consturcted raw corpus to train a code tokenizer using the following command.  \n",
    "\n",
    "The command to train the code tokenizer for the pre-processed datasets will be:  \n",
    "`spm_train --input=/home/BPE/code.txt --model_type=bpe --vocab_size=25000 --model_prefix=code_bpe_25K --bos_id=0 --pad_id=1 --pad_piece=[PAD] --eos_id=2 --unk_id=3`  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47804be",
   "metadata": {},
   "source": [
    "Statistics about the raw corpus (functions.txt) of the pre-training dataset are shown below.  \n",
    "   | Total number of tasks    | Total number of Java functions    | Total number of Python functions    |   \n",
    "   | :----------               | :----------        | :----------       |     \n",
    "   | 2001    | 17,785         |  15,399        |  \n",
    "\n",
    "Statistics about the raw corpus (programs.txt) of the fine-tuning dataset are shown below. \n",
    "   | Total number of Java tasks | Total number of Java programs | Total number of Python tasks | Total number of Python programs    |   \n",
    "   | :----------               | :----------        | :----------               | :----------       |     \n",
    "   | 993    | 5660   |  1007   |  6239    |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42a1b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "from pprint import pprint\n",
    "\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "597aa062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an one-sentence-per-line raw corpus 'code.txt' based on the 'functions.txt' and the 'programs.txt'\n",
    "\n",
    "functions_txt_path = '/Users/rongdang/Desktop/semantic-code-clone/dataset/cross-language/CodeNet_Microsoft/preprocessed_dataset/functions.txt'\n",
    "programs_txt_path = '/Users/rongdang/Desktop/semantic-code-clone/dataset/cross-language/C4/preprocessed_dataset/programs.txt'\n",
    "\n",
    "code_txt_path = '/Users/rongdang/Desktop/semantic-code-clone/checkpoint/BPE_Model/code.txt'\n",
    "code_lines = list()\n",
    "\n",
    "with open(functions_txt_path, mode='r', encoding='utf-8') as function_txt_file:\n",
    "    function_lines = function_txt_file.readlines()\n",
    "    code_lines.extend(function_lines)\n",
    "function_txt_file.close()\n",
    "\n",
    "with open(programs_txt_path, mode='r', encoding='utf-8') as program_txt_file:\n",
    "    program_lines = program_txt_file.readlines()\n",
    "    code_lines.extend(program_lines)\n",
    "program_txt_file.close()\n",
    "\n",
    "with open(code_txt_path, mode='w') as code_txt_file:\n",
    "    code_txt_file.writelines(code_lines)\n",
    "code_txt_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4508ecf9",
   "metadata": {},
   "source": [
    "### Part 2: Usage of the SentencePiece Model\n",
    "We will display tokenization results of one Java-Python function pair from the pre-training dataset and one Java-Python program pair from the fine-tuning dataset using the pre-trained SentencePiece BPE tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee182e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the path of sentencepiece models\n",
    "\n",
    "spm_model_path = '/Users/rongdang/Desktop/semantic-code-clone/checkpoint/BPE_Model/code_bpe_25K.model'\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(spm_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac4c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path of pre-training dataset\n",
    "\n",
    "ccs_root = '/Users/rongdang/Desktop/semantic-code-clone/dataset/cross-language/CodeNet_Microsoft'\n",
    "correct_functions = os.path.join(ccs_root, 'preprocessed_dataset', 'correct_functions.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "914a135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path of fine-tuning dataset\n",
    "\n",
    "c4_root = '/Users/rongdang/Desktop/semantic-code-clone/dataset/cross-language/C4'\n",
    "correct_programs = os.path.join(c4_root, 'preprocessed_dataset', 'correct_programs.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baa08735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample Java function: \n",
      "('import java.util.Scanner;\\n'\n",
      " 'public class Main\\n'\n",
      " '{\\n'\n",
      " '    public static void main(String[] args)\\n'\n",
      " '    {\\n'\n",
      " '        Scanner scan = new Scanner(System.in);\\n'\n",
      " '        int n = scan.nextInt();\\n'\n",
      " '        int s = scan.nextInt();\\n'\n",
      " '        int b = scan.nextInt();\\n'\n",
      " '        while (n != 0 && s != 0 && b != 0) {\\n'\n",
      " '            int r = 0;\\n'\n",
      " '            int f = 0;\\n'\n",
      " '            int t[] = new int[n];\\n'\n",
      " '            for (int i = 0; i < n; i++) {\\n'\n",
      " '                t[i] = scan.nextInt();\\n'\n",
      " '            }\\n'\n",
      " '            for (int x = s; x <= b; x++) {\\n'\n",
      " '                int l = t[x - 1] - t[x];\\n'\n",
      " '                if (r <= l) {\\n'\n",
      " '                    r = l;\\n'\n",
      " '                    f = x;\\n'\n",
      " '                }\\n'\n",
      " '            }\\n'\n",
      " '            System.out.println(f);\\n'\n",
      " '            n = scan.nextInt();\\n'\n",
      " '            s = scan.nextInt();\\n'\n",
      " '            b = scan.nextInt();\\n'\n",
      " '        }\\n'\n",
      " '    }\\n'\n",
      " '}')\n",
      "sample Java tokens: \n",
      "['▁import', '▁java', '.', 'util', '.', 'Scanner', ';', '▁public', '▁class', '▁Main', '▁{', '▁public', '▁static', '▁void', '▁main', '(', 'String', '[]', '▁args', ')', '▁{', '▁Scanner', '▁scan', '▁=', '▁new', '▁Scanner', '(', 'System', '.', 'in', ');', '▁int', '▁n', '▁=', '▁scan', '.', 'nextInt', '();', '▁int', '▁s', '▁=', '▁scan', '.', 'nextInt', '();', '▁int', '▁b', '▁=', '▁scan', '.', 'nextInt', '();', '▁while', '▁(', 'n', '▁!=', '▁0', '▁&&', '▁s', '▁!=', '▁0', '▁&&', '▁b', '▁!=', '▁0)', '▁{', '▁int', '▁r', '▁=', '▁0;', '▁int', '▁f', '▁=', '▁0;', '▁int', '▁t', '[]', '▁=', '▁new', '▁int', '[', 'n', '];', '▁for', '▁(', 'int', '▁i', '▁=', '▁0;', '▁i', '▁<', '▁n', ';', '▁i', '++)', '▁{', '▁t', '[', 'i', ']', '▁=', '▁scan', '.', 'nextInt', '();', '▁}', '▁for', '▁(', 'int', '▁x', '▁=', '▁s', ';', '▁x', '▁<=', '▁b', ';', '▁x', '++)', '▁{', '▁int', '▁l', '▁=', '▁t', '[', 'x', '▁-', '▁1]', '▁-', '▁t', '[', 'x', '];', '▁if', '▁(', 'r', '▁<=', '▁l', ')', '▁{', '▁r', '▁=', '▁l', ';', '▁f', '▁=', '▁x', ';', '▁}', '▁}', '▁System', '.', 'out', '.', 'println', '(', 'f', ');', '▁n', '▁=', '▁scan', '.', 'nextInt', '();', '▁s', '▁=', '▁scan', '.', 'nextInt', '();', '▁b', '▁=', '▁scan', '.', 'nextInt', '();', '▁}', '▁}', '▁}']\n",
      "------------------------------------------------------\n",
      "sample Python function: \n",
      "('def solve(a,b,c):\\n'\n",
      " '    scores = [int(input()) for i in range(a)]\\n'\n",
      " '    gap = scores[b-1] - scores[b]\\n'\n",
      " '    tmpans = b-1\\n'\n",
      " '    for i in range(b-1,c):\\n'\n",
      " '        tmpgap = scores[i] - scores[i+1]\\n'\n",
      " '        if gap <= tmpgap:\\n'\n",
      " '            tmpans = i\\n'\n",
      " '            gap = tmpgap\\n'\n",
      " '    print(tmpans+1)\\n'\n",
      " 'while True:\\n'\n",
      " '    a,b,c = map(int,input().split())\\n'\n",
      " '    if a == b == c and a == 0:\\n'\n",
      " '        exit()\\n'\n",
      " '    else:\\n'\n",
      " '        solve(a,b,c)')\n",
      "sample Python tokens: \n",
      "['▁def', '▁solve', '(', 'a', ',', 'b', ',', 'c', '):', '▁scores', '▁=', '▁[', 'int', '(', 'input', '())', '▁for', '▁i', '▁in', '▁range', '(', 'a', ')]', '▁gap', '▁=', '▁scores', '[', 'b', '-1]', '▁-', '▁scores', '[', 'b', ']', '▁tmpans', '▁=', '▁b', '-1', '▁for', '▁i', '▁in', '▁range', '(', 'b', '-1,', 'c', '):', '▁tmp', 'gap', '▁=', '▁scores', '[', 'i', ']', '▁-', '▁scores', '[', 'i', '+1]', '▁if', '▁gap', '▁<=', '▁tmp', 'gap', ':', '▁tmpans', '▁=', '▁i', '▁gap', '▁=', '▁tmp', 'gap', '▁print', '(', 'tmp', 'ans', '+1)', '▁while', '▁True', ':', '▁a', ',', 'b', ',', 'c', '▁=', '▁map', '(', 'int', ',', 'input', '().', 'split', '())', '▁if', '▁a', '▁==', '▁b', '▁==', '▁c', '▁and', '▁a', '▁==', '▁0:', '▁exit', '()', '▁else', ':', '▁solve', '(', 'a', ',', 'b', ',', 'c', ')']\n"
     ]
    }
   ],
   "source": [
    "# display the tokenization result of one Java-Python function pair from the pre-training dataset\n",
    "\n",
    "with open(correct_functions, mode='r', encoding='utf-8') as correct_functions_file:\n",
    "    json_data = json.load(correct_functions_file)\n",
    "    java_pool = json_data['java']\n",
    "    python_pool = json_data['python']\n",
    "    \n",
    "    java_code = java_pool['1085']['s311391346']\n",
    "    java_tokens = sp.Encode(java_code, out_type=str)\n",
    "    \n",
    "    python_code = python_pool['1085']['s860127205']\n",
    "    python_tokens = sp.Encode(python_code, out_type=str)\n",
    "\n",
    "correct_functions_file.close()\n",
    "\n",
    "print('sample Java function: ')\n",
    "pprint(java_code)\n",
    "print('sample Java tokens: ')\n",
    "print(java_tokens)\n",
    "print('------------------------------------------------------')\n",
    "print('sample Python function: ')\n",
    "pprint(python_code)\n",
    "print('sample Python tokens: ')\n",
    "print(python_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58aff46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample Java program: \n",
      "('import java.util.*; \\n'\n",
      " 'public class Main { \\n'\n",
      " ' public static void main (String[] args) { \\n'\n",
      " '  Scanner sc = new Scanner(System.in); \\n'\n",
      " '  int n = sc.nextInt(); \\n'\n",
      " '  if (isPrime(n)) { \\n'\n",
      " '   System.out.println(\"YES\"); \\n'\n",
      " '  } else { \\n'\n",
      " '   System.out.println(\"NO\"); \\n'\n",
      " '  } \\n'\n",
      " ' } \\n'\n",
      " ' static boolean isPrime(int x) { \\n'\n",
      " '  for (int i = 2; i <= 1000 && i < x; i++) { \\n'\n",
      " '   if (x % i == 0) { \\n'\n",
      " '    return false; \\n'\n",
      " '   } \\n'\n",
      " '  } \\n'\n",
      " '  return true; \\n'\n",
      " ' } \\n'\n",
      " '}')\n",
      "sample Java tokens: \n",
      "['▁import', '▁java', '.', 'util', '.*;', '▁public', '▁class', '▁Main', '▁{', '▁public', '▁static', '▁void', '▁main', '▁(', 'String', '[]', '▁args', ')', '▁{', '▁Scanner', '▁sc', '▁=', '▁new', '▁Scanner', '(', 'System', '.', 'in', ');', '▁int', '▁n', '▁=', '▁sc', '.', 'nextInt', '();', '▁if', '▁(', 'isPrime', '(', 'n', '))', '▁{', '▁System', '.', 'out', '.', 'println', '(\"', 'YES', '\");', '▁}', '▁else', '▁{', '▁System', '.', 'out', '.', 'println', '(\"', 'NO', '\");', '▁}', '▁}', '▁static', '▁boolean', '▁isPrime', '(', 'int', '▁x', ')', '▁{', '▁for', '▁(', 'int', '▁i', '▁=', '▁2;', '▁i', '▁<=', '▁1000', '▁&&', '▁i', '▁<', '▁x', ';', '▁i', '++)', '▁{', '▁if', '▁(', 'x', '▁%', '▁i', '▁==', '▁0)', '▁{', '▁return', '▁false', ';', '▁}', '▁}', '▁return', '▁true', ';', '▁}', '▁}']\n",
      "------------------------------------------------------\n",
      "sample Python program: \n",
      "('import math \\n'\n",
      " 'def is_prime(n): \\n'\n",
      " '    if n == 1: \\n'\n",
      " '        return False \\n'\n",
      " '    for k in range(2, int(math.sqrt(n)) + 1): \\n'\n",
      " '        if n % k == 0: \\n'\n",
      " '            return False \\n'\n",
      " '    return True \\n'\n",
      " 'N = int(input()) \\n'\n",
      " 'if is_prime(N): \\n'\n",
      " \"    print('YES') \\n\"\n",
      " 'else: \\n'\n",
      " \"    print('NO')\")\n",
      "sample Python tokens: \n",
      "['▁import', '▁math', '▁def', '▁is', '_', 'prime', '(', 'n', '):', '▁if', '▁n', '▁==', '▁1:', '▁return', '▁False', '▁for', '▁k', '▁in', '▁range', '(2,', '▁int', '(', 'math', '.', 'sqrt', '(', 'n', '))', '▁+', '▁1):', '▁if', '▁n', '▁%', '▁k', '▁==', '▁0:', '▁return', '▁False', '▁return', '▁True', '▁N', '▁=', '▁int', '(', 'input', '())', '▁if', '▁is', '_', 'prime', '(', 'N', '):', '▁print', \"('\", 'YES', \"')\", '▁else', ':', '▁print', \"('\", 'NO', \"')\"]\n"
     ]
    }
   ],
   "source": [
    "# display the tokenization result of one Java-Python program pair from the fine-tuning dataset\n",
    "\n",
    "with open(correct_programs, mode='r', encoding='utf-8') as correct_programs_file:\n",
    "    json_data = json.load(correct_programs_file)\n",
    "    java_pool = json_data['java']\n",
    "    python_pool = json_data['python']\n",
    "    \n",
    "    java_code = java_pool['1245']['79154']\n",
    "    java_tokens = sp.Encode(java_code, out_type=str)\n",
    "    \n",
    "    python_code = python_pool['1245']['79131']\n",
    "    python_tokens = sp.Encode(python_code, out_type=str)\n",
    "\n",
    "correct_programs_file.close()\n",
    "\n",
    "print('sample Java program: ')\n",
    "pprint(java_code)\n",
    "print('sample Java tokens: ')\n",
    "print(java_tokens)\n",
    "print('------------------------------------------------------')\n",
    "print('sample Python program: ')\n",
    "pprint(python_code)\n",
    "print('sample Python tokens: ')\n",
    "print(python_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b9bbce",
   "metadata": {},
   "source": [
    "### Part 3: Analyze the Pre-training and the Fine-tuning Dataset\n",
    "We will investigate dataset statistics in terms of: 1) the average number of code lines for each language; 2) the average number of code tokens for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72a33b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_code_lines(code_str):\n",
    "    line_num = 0\n",
    "    code_lines = code_str.split('\\n')\n",
    "    for line in code_lines:\n",
    "        line = line.strip('\\n').strip('\\t').strip()\n",
    "        if line != '\\n' and line != '\\t' and line != '':\n",
    "            line_num += 1\n",
    "    return line_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f0a8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Pre-training dataset statistics-----\n",
      "avg number of lines in Java function: 110.94\n",
      "avg number of code sub-tokens in Java function: 821.29\n",
      "avg number of lines in Python function: 22.93\n",
      "avg number of code sub-tokens in Python function: 178.91\n"
     ]
    }
   ],
   "source": [
    "# analyze the dataset statistics of the pre-training dataset using the SentencePiece BPE tokenizer\n",
    "\n",
    "java_line_nums = list()\n",
    "java_token_nums = list()\n",
    "python_line_nums = list()\n",
    "python_token_nums = list()\n",
    "\n",
    "with open(correct_functions, mode='r', encoding='utf-8') as correct_functions_file:\n",
    "    json_data = json.load(correct_functions_file)\n",
    "    java_pool = json_data['java']\n",
    "    python_pool = json_data['python']\n",
    "    \n",
    "    for task, solutions in java_pool.items():\n",
    "        for _, solution in solutions.items():\n",
    "            java_line_num = count_code_lines(solution)\n",
    "            java_token_num = len(sp.Encode(solution, out_type=str))\n",
    "            java_line_nums.append(java_line_num)\n",
    "            java_token_nums.append(java_token_num)\n",
    "            \n",
    "    for task, solutions in python_pool.items():\n",
    "        for _, solution in solutions.items():\n",
    "            python_line_num = count_code_lines(solution)\n",
    "            python_token_num = len(sp.Encode(solution, out_type=str))\n",
    "            python_line_nums.append(python_line_num)\n",
    "            python_token_nums.append(python_token_num)\n",
    "            \n",
    "correct_functions_file.close()\n",
    "\n",
    "print('-----Pre-training dataset statistics-----')\n",
    "print('avg number of lines in Java function: ' + str(round(sum(java_line_nums)/len(java_line_nums), 2)))\n",
    "print('avg number of code sub-tokens in Java function: ' + str(round(sum(java_token_nums)/len(java_token_nums), 2)))\n",
    "print('avg number of lines in Python function: ' + str(round(sum(python_line_nums)/len(python_line_nums), 2)))\n",
    "print('avg number of code sub-tokens in Python function: ' + str(round(sum(python_token_nums)/len(python_token_nums), 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c924f0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Fine-tuning dataset statistics-----\n",
      "avg number of lines in Java program: 64.49\n",
      "avg number of code sub-tokens in Java program: 426.73\n",
      "avg number of lines in Python program: 20.69\n",
      "avg number of code sub-tokens in Python program: 171.53\n"
     ]
    }
   ],
   "source": [
    "# analyze the dataset statistics of the fine-tuning dataset using the SentencePiece BPE tokenizer\n",
    "\n",
    "java_line_nums = list()\n",
    "java_token_nums = list()\n",
    "python_line_nums = list()\n",
    "python_token_nums = list()\n",
    "\n",
    "with open(correct_programs, mode='r', encoding='utf-8') as correct_programs_file:\n",
    "    json_data = json.load(correct_programs_file)\n",
    "    java_pool = json_data['java']\n",
    "    python_pool = json_data['python']\n",
    "    \n",
    "    for task, solutions in java_pool.items():\n",
    "        for _, solution in solutions.items():\n",
    "            java_line_num = count_code_lines(solution)\n",
    "            java_token_num = len(sp.Encode(solution, out_type=str))\n",
    "            java_line_nums.append(java_line_num)\n",
    "            java_token_nums.append(java_token_num)\n",
    "            \n",
    "    for task, solutions in python_pool.items():\n",
    "        for _, solution in solutions.items():\n",
    "            python_line_num = count_code_lines(solution)\n",
    "            python_token_num = len(sp.Encode(solution, out_type=str))\n",
    "            python_line_nums.append(python_line_num)\n",
    "            python_token_nums.append(python_token_num)\n",
    "            \n",
    "correct_programs_file.close()\n",
    "\n",
    "print('-----Fine-tuning dataset statistics-----')\n",
    "print('avg number of lines in Java program: ' + str(round(sum(java_line_nums)/len(java_line_nums), 2)))\n",
    "print('avg number of code sub-tokens in Java program: ' + str(round(sum(java_token_nums)/len(java_token_nums), 2)))\n",
    "print('avg number of lines in Python program: ' + str(round(sum(python_line_nums)/len(python_line_nums), 2)))\n",
    "print('avg number of code sub-tokens in Python program: ' + str(round(sum(python_token_nums)/len(python_token_nums), 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a0963e",
   "metadata": {},
   "source": [
    "### analyze the dataset statistics of the pre-training dataset using the pre-trained CodeBERT tokenizer\n",
    "-----Pre-training dataset statistics-----  \n",
    "avg number of lines in Java function: 110.94  \n",
    "avg number of code tokens in Java function: 935.5  \n",
    "avg number of lines in Python function: 22.93  \n",
    "avg number of code tokens in Python function: 209.86  \n",
    "\n",
    "### analyze the dataset statistics of the fine-tuning dataset using the pre-trained CodeBERT tokenizer\n",
    "-----Fine-tuning dataset statistics-----  \n",
    "avg number of lines in Java program: 64.49  \n",
    "avg number of code tokens in Java program: 496.59  \n",
    "avg number of lines in Python program: 20.69  \n",
    "avg number of code tokens in Python program: 213.93  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
