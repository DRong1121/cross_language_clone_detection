{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae050d0c",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391fd52b",
   "metadata": {},
   "source": [
    "### CodeSearchNet dataset:\n",
    "CodeSearchNet, a large corpus of methods extracted from popular GitHub repositories. The primary dataset consists of 2 million (comment, code) pairs from open-source libraries. Concretely, a 'comment' is a top-level function or method, and 'code' is an entire function or method. Currently, the dataset contains Python, Javascript, Ruby, Go, Java and PHP functions. The dataset is partitioned into train, validation and test sets such that code from the same repository can only exist in one partition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa34ef8",
   "metadata": {},
   "source": [
    "For the CodeSearchNet dataset cleaned and released by Microsoft, we intend to take a look at the data at the function level in Java and Python.  \n",
    "Different from the original CodeSearchNet, the answer of each query is retrieved from the whole development and testing code corpus instead of 1,000 candidate codes. Besides, some queries contain content unrelated to the code, such as a link that refers to external resources. Therefore, the dataset is filtered to improve the quality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32894c2",
   "metadata": {},
   "source": [
    "Statistics about the cleaned CodeSearchNet dataset released by Microsoft are shown below.  \n",
    "   | Programming Language      | Train     | Valid      | Test     | Candidate codes    |   \n",
    "   | :---------- | :---------- | :---------- | :---------- | :----------  |     \n",
    "   | Java  |164,923      | 5,183         | 10,955         | 13,981        |  \n",
    "   | Python  |251,820    | 13,914        | 14,918         | 42,827        |    \n",
    "\n",
    "For the CodeSearchNet dataset, we intend to explore Java and Python functions. First, we will apply the TransCoder model (https://github.com/facebookresearch/CodeGen/blob/main/docs/transcoder.md#pre-trained-models) to translate Java functions into their Python counterparts. Then, we will filter out unparsable transfered Python functions. Finally, we will construct Java-Python functions pairs and investigate the generated dataset statistics in terms of: 1) the average number of code lines and 2) the average number of code tokens in each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f6a1394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import jsonlines\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 300)\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d42f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define csn_generated dir\n",
    "\n",
    "csn_generated = '/Users/rongdang/Desktop/semantic-code-clone/dataset/pre-training/CSN-Generated'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a10dd5",
   "metadata": {},
   "source": [
    "### Part 1: Preview the generated dataset\n",
    "1)We applied the TransCoder_DOBF model to translate Java functions from the CSN dataset into corresponding Python functions. 2)We filtered out unparsable transfered Python functions. Finally, we generated 178087 Java-Python function pairs. We constructed the generated dataset and splitted it into 2 separate sets: the training set (function pairs from the original training set) and the validation set (function pairs from the original valid and test set). The ratio of #num of train: #num of valid in the generated dataset is approximately 9:1 (162174: 15913).  \n",
    "The augmented pre-training dataset contains the original pre-training dataset and the generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3bef8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'java-python',\n",
      " 'src_code': 'public Set<BsonValue> getPausedDocumentIds(final MongoNamespace '\n",
      "             'namespace) {\\n'\n",
      "             '    this.waitUntilInitialized();\\n'\n",
      "             '\\n'\n",
      "             '    try {\\n'\n",
      "             '      ongoingOperationsGroup.enter();\\n'\n",
      "             '      final Set<BsonValue> pausedDocumentIds = new HashSet<>();\\n'\n",
      "             '\\n'\n",
      "             '      for (final CoreDocumentSynchronizationConfig config :\\n'\n",
      "             '          this.syncConfig.getSynchronizedDocuments(namespace)) '\n",
      "             '{\\n'\n",
      "             '        if (config.isPaused()) {\\n'\n",
      "             '          pausedDocumentIds.add(config.getDocumentId());\\n'\n",
      "             '        }\\n'\n",
      "             '      }\\n'\n",
      "             '\\n'\n",
      "             '      return pausedDocumentIds;\\n'\n",
      "             '    } finally {\\n'\n",
      "             '      ongoingOperationsGroup.exit();\\n'\n",
      "             '    }\\n'\n",
      "             '  }',\n",
      " 'task_id': '154879',\n",
      " 'tgt_code': 'def get_paused_document_ids ( cls , namespace ) :\\n'\n",
      "             '    cls.__wait_until_initialized ( )\\n'\n",
      "             '    try :\\n'\n",
      "             '        cls.__ongoing_operations_group.enter ( )\\n'\n",
      "             '        paused_document_ids = set ( )\\n'\n",
      "             '        for config in '\n",
      "             'cls.__sync_config.get_synchronous_documents ( namespace ) :\\n'\n",
      "             '            if config.is_paused :\\n'\n",
      "             '                paused_document_ids.add ( config.document_id )\\n'\n",
      "             '        return paused_document_ids\\n'\n",
      "             '    finally :\\n'\n",
      "             '        cls.__wait_until_initialized ( )'}\n"
     ]
    }
   ],
   "source": [
    "# display one Java-Python pair in the generated dataset\n",
    "\n",
    "sets = ['train', 'valid']\n",
    "\n",
    "file_name = 'pair_' + sets[0] + '.jsonl'\n",
    "file_path = os.path.join(csn_generated, file_name)\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    sample_file = f.readlines()\n",
    "sample_data = json.loads(sample_file[0])\n",
    "pprint(sample_data)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b6d586",
   "metadata": {},
   "source": [
    "### Part 2: Analyze the generated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123d681e",
   "metadata": {},
   "source": [
    "### analyze the dataset statistics of the CSN-Generated dataset using the pre-trained CodeBERT tokenizer\n",
    "\n",
    "-----CSN-Generated dataset statistics-----  \n",
    "avg number of lines in Java function: 14.26  \n",
    "avg number of code tokens in Java function: 137.03  \n",
    "avg number of lines in Python function: 9.1  \n",
    "avg number of code tokens in Python function: 111.31  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
